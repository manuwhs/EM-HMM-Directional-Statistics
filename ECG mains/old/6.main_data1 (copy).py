
# Official libraries
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import copy
# Own libraries
import import_folders
from graph_lib import gl


import sampler_lib as sl
import EM_lib as EMl
import EM_libfunc as EMlf
import HMM_lib as HMMl
import HMM_libfunc2 as HMMlf
import decoder_lib as decl
import pickle_lib as pkl
import scipy.io
from sklearn import preprocessing

import Watson_distribution as Wad
import Watson_sampling as Was
import Watson_estimators as Wae
import general_func as gf

plt.close("all")

################################################################
######## Load the dataset ! ##############################
###############################################################

load_dataset = 0

if (load_dataset):
    dataset_folder = "./dataset/"
    mat = scipy.io.loadmat(dataset_folder +'face_scrambling_spm8proc_sub07.mat')
    keys = mat.keys()
    print keys
    X = mat["X"]   # Nchannels x Time x Ntrials
    trial_indices = mat["trial_indices"][0][0]  # Labels of the trials
    label_classes = ["Famous", "Unfamiliar", "Scrambled"]
    Nclasses = len(label_classes)
    label_numbers = range(Nclasses)
    # Now X_All_labels has in every postion the trials for each class in the form
    # of a matrix Ntrials x Ntimes x Ndim

################################################################
######## Loading in lists and preprocessing! ##############################
###############################################################
preprocess_data = 0
if (preprocess_data):
    max_trials = 100
    i0 = 0  # For selecting subset of dimensions
    ndim = 70
    Ks_params = []  
    
    X_data_trials = []  # List of trials :)
    X_data_labels = []  # Labels of the trials :)

    # Every label will have set of clusters, 
    # this is the parameters of the clusters [K][[pimix], [thetas]]
    
    ############## Separate by classes in lists ###############
    X_All_labels = []  # Contains the trials for every label
    for label in label_classes:
        label_trials = trial_indices[label].flatten()
        X_label_trials = X[:,:,np.where(label_trials == 1)[0]].T
        X_All_labels.append(X_label_trials)
    
    ############# Preprocess by class ? ###################
    for i in range(Nclasses):
        Ntrials, Nsamples, Ndim = X_All_labels[i].shape
        
        # Limit the number of trials processed by labels
        Ntrials = np.min([Ntrials,max_trials])
        
        for nt in range(Ntrials):
            ################################################################
            ######## Preprocessing ! ##############################
            ###############################################################
            X_trial = X_All_labels[i][nt,:,:]
            X_trial = X_trial[:,i0:i0 +ndim]
#            X_trial = X_trial - np.sum(X_trial, axis = 1).reshape(X_trial.shape[0],1)
    #        scaler = preprocessing.StandardScaler().fit(X_trial)
    #        X_trial = scaler.transform(X_trial)            
            X_trial = gf.normalize_data(X_trial)
            
            X_data_trials.append(X_trial)
            X_data_labels.append(i)
            
    # Now have a normal machine learning problem :)
    # X_data_trials,  X_data_labels
    ################# Separate in train and validation ############
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(X_data_trials, X_data_labels, test_size=0.50, random_state = 0, stratify = X_data_labels)

## Plot this shit 
#gl.scatter_3D(0, 0,0, nf = 1, na = 0)
#for i in range(Nclasses):
#    X_train_class_i = [ X_train[j] for j in np.where(np.array(y_train) == i)[0]]
#    X_train_class_i = np.concatenate(X_train_class_i, axis = 0)
#    gl.scatter_3D(X_train_class_i[:,0], X_train_class_i[:,1],X_train_class_i[:,2], nf = 0, na = 0)
# 
####################################################### 
######################### EM ########################### 
####################################################### 

EM_flag = 0
if (EM_flag):
    Ninit = 20
    K  =  5
    verbose = 1
    T  = 50
    for i in range(Nclasses):
        X_train_class_i = [ X_train[j] for j in np.where(np.array(y_train) == i)[0]]
        logl,theta_list,pimix_list = EMl.run_several_EM(X_train_class_i, K = K, delta = 0.1, T = T,
                                    Ninit = Ninit, verbose = verbose)
        Ks_params.append([pimix_list[-1],theta_list[-1]])
    

    # To store the likelihoods that the sequence was generated by the clusters
    Likelihoods = [] # np.zeros(1000,(len(Ks_params)))
        # Loop over the data points
    
    ### Calculate train accuracy
    for trial_i in range(len(X_train)): 
            likeli = []
            for K_p_i in range(Nclasses):   # For every cluster type
                ll = EMlf.get_EM_Incomloglike_log(Ks_params[K_p_i][1],Ks_params[K_p_i][0],X_train[trial_i])
                likeli.append(ll)
            Likelihoods.append(likeli)
    
    Likelihoods = np.array(Likelihoods)
    print y_train
    print np.argmax(Likelihoods, axis = 1)
    print "Train Accuracy %f" %(gf.accuracy(y_train, np.argmax(Likelihoods, axis = 1)))

    Likelihoods = []
    ### Calculate test accuracy
    for trial_i in range(len(X_test)): 
            likeli = []
            for K_p_i in range(Nclasses):   # For every cluster type
                ll = EMlf.get_EM_Incomloglike_log(Ks_params[K_p_i][1],Ks_params[K_p_i][0],X_test[trial_i])
                likeli.append(ll)
            Likelihoods.append(likeli)
    
    Likelihoods = np.array(Likelihoods)
    print y_test
    print np.argmax(Likelihoods, axis = 1)
    print "Test Accuracy %f" %(gf.accuracy(y_test, np.argmax(Likelihoods, axis = 1)))

####################################################### 
######################### HMM ########################### 
####################################################### 
HMM_flag = 0
if (HMM_flag):
    Nit = 1
    
    Is_params = []
    for k in range(len(label_classes)): 
        EM_params = Ks_params[k]
        pimix = EM_params[0]
        theta = EM_params[1]
        
        I = pimix.size
        
        pi_init = pimix
        B_init = theta
        A_init = np.repeat(pi_init, I, axis = 0)
    
        X_train_class_k = [ X_train[j] for j in np.where(np.array(y_train) == k)[0]]
        logl,B_list,pi_list, A_list = \
            HMMl.run_several_HMM(data = X_train_class_k,I = I,delta = 0.01, R = 20
                     ,pi_init = pi_init, A_init = A_init, B_init = B_init, Ninit = Nit)
    
        Is_params.append([pi_list[-1],A_list[-1], B_list[-1]])

    # To store the likelihoods that the sequence was generated by the clusters
    Likelihoods = [] # np.zeros(1000,(len(Ks_params)))
        # Loop over the data points
    
    for trial_i in range(len(X_train)): 
            likeli = []
            for K_p_i in range(Nclasses):   # For every cluster type
                ll = HMMlf.get_HMM_Incomloglike(Is_params[K_p_i][1],  # A,B,pi
                                                Is_params[K_p_i][2],
                                                Is_params[K_p_i][0],
                                                [X_train[trial_i]])
                likeli.append(ll)
            Likelihoods.append(likeli)
    
    Likelihoods = np.array(Likelihoods)
    
    print y_test
    print np.argmax(Likelihoods, axis = 1)
    print "Train Accuracy %f" %(gf.accuracy(y_train, np.argmax(Likelihoods, axis = 1)))
    
      
    # To store the likelihoods that the sequence was generated by the clusters
    Likelihoods = [] # np.zeros(1000,(len(Ks_params)))
        # Loop over the data points
        
    for trial_i in range(len(X_test)): 
            likeli = []
            for K_p_i in range(Nclasses):   # For every cluster type
                ll = HMMlf.get_HMM_Incomloglike(Is_params[K_p_i][1],  # A,B,pi
                                                Is_params[K_p_i][2],
                                                Is_params[K_p_i][0],
                                                [X_test[trial_i]])
                likeli.append(ll)
            Likelihoods.append(likeli)
    
    Likelihoods = np.array(Likelihoods)
    
    print y_test
    print np.argmax(Likelihoods, axis = 1)
    print "Test Accuracy %f" %(gf.accuracy(y_test, np.argmax(Likelihoods, axis = 1)))

